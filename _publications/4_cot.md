---
title: "Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers"
collection: publications
excerpt: 'Chain-of-thought reasoning and scratchpads have emerged as critical tools for enhancing the computational capabilities of transformers. While theoretical results show that polynomial-length scratchpads can extend transformersâ€™ expressivity from TC0 to PTIME, their required length remains poorly understood. Empirical evidence even suggests that transformers need scratchpads even for many problems in TC0, such as Parity or Multiplication, challenging optimistic bounds derived from circuit complexity. In this work, we initiate the study of systematic lower bounds for the number of CoT steps across different algorithmic problems, in the hard-attention regime. We study a variety of algorithmic problems, and provide bounds that are tight up to logarithmic factors. Overall, these results contribute to emerging understanding of the power and limitations of chain-of-thought reasoning.'
date: 2025-02-04
venue: 'arXiv'
paperurl: 'https://arxiv.org/pdf/2502.02393'
citation: 'Amiri, A., Huang, X., Rofin, M. and Hahn, M., 2025. Lower Bounds for Chain-of-Thought Reasoning in Hard-Attention Transformers. arXiv preprint arXiv:2502.02393'
authors: 'Alireza Amiri, Xinting Huang, Mark Rofin, Michael Hahn.'
---