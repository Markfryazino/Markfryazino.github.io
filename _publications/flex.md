---
title: "Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models"
collection: publications
permalink: /publication/linear_interpolation
excerpt: 'The simplest way to obtain continuous interpolation between two points in high dimensional space is to draw a line between them. While previous works focused on the general connectivity between model parameters, we explored linear interpolation for parameters of pre-trained models after fine-tuning. Surprisingly, we could perform linear interpolation without a performance drop in intermediate points for fine-tuned models. For controllable text generation, such interpolation could be seen as moving a model towards or against the desired text attribute (e.g., positive sentiment), which could be used as grounds for further methods for controllable text generation without inference speed overhead.'
date: 2022-11-22
venue: 'arXiv'
paperurl: 'https://arxiv.org/pdf/2211.12092.pdf'
citation: 'Rofin, Mark, Nikita Balagansky, and Daniil Gavrilov. 2022. Linear Interpolation In Parameter Space is Good Enough for Fine-Tuned Language Models. arXiv preprint arXiv:2211.12092'
---

The simplest way to obtain continuous interpolation between two points in high dimensional space is to draw a line between them. While previous works focused on the general connectivity between model parameters, we explored linear interpolation for parameters of pre-trained models after fine-tuning. Surprisingly, we could perform linear interpolation without a performance drop in intermediate points for fine-tuned models. For controllable text generation, such interpolation could be seen as moving a model towards or against the desired text attribute (e.g., positive sentiment), which could be used as grounds for further methods for controllable text generation without inference speed overhead.

[Download paper here](https://arxiv.org/pdf/2211.12092.pdf)